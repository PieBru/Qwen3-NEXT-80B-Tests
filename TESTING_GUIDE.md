# Comprehensive Testing Guide for Qwen3-80B MoE Implementation

**‚úÖ STATUS: FULLY WORKING - Model loading, caching, and inference confirmed operational!**

This guide will help you test and experiment with the Qwen3-80B MoE BitsAndBytes implementation, which is now fully functional with fast cached loading (~22 seconds) and successful inference.

## üéâ Current Working Status

- **Model Loading**: ‚úÖ Fixed - Loads from cache in ~22 seconds
- **First Load**: ~8-10 minutes (creating cache)
- **Inference**: ‚úÖ Working - Successfully tested ("2+2=4")
- **Caching**: ‚úÖ Enabled - 40GB cache file for instant restarts
- **API Server**: ‚úÖ Operational - All endpoints functional

## ‚ö†Ô∏è Pre-Testing Checklist

Before starting any tests, ensure:

- [ ] **Model downloaded**: Check with `python src/download_model.py` (~40GB)
- [ ] **Virtual environment activated**: `source .venv/bin/activate`
- [ ] **GPU available**: Run `nvidia-smi` to verify CUDA GPU
- [ ] **Sufficient memory**: Need ~55-60GB RAM available (model uses CPU execution)
- [ ] **Dependencies installed**: Try `python -c "import torch; print(torch.cuda.is_available())"`
- [ ] **Time allocated**: First model load takes 8-10 minutes (subsequent loads: ~22 seconds)

If any check fails, see the Installation section below.

‚ö° **FAST LOADING**: After first load, model caching enables ~22 second restart time!

## üìã Prerequisites

### System Requirements
- **GPU**: NVIDIA RTX 4090 or similar (8GB+ VRAM used for embeddings)
- **RAM**: 55-60GB+ available system memory (model runs mostly on CPU)
- **OS**: Linux (tested on Arch Linux)
- **CUDA**: 11.8 or higher
- **Python**: 3.9+
- **Disk Space**: ~80GB total (40GB model + 40GB cache)

### Verify System Resources
```bash
# Check GPU and VRAM
nvidia-smi

# Check available RAM
free -h

# Check CUDA version
nvcc --version

# Check disk space
df -h .
```

## üõ†Ô∏è Installation

### Step 1: Clone and Setup
```bash
# Clone the repository
git clone https://github.com/PieBru/Qwen3-NEXT-80B-Tests.git
cd Qwen3-80B_test

# Make scripts executable
chmod +x install.sh run.sh

# Install dependencies (creates .venv automatically)
./install.sh
```

### Step 2: Activate Virtual Environment
```bash
# The project uses `uv` and .venv (not venv)
uv venv
source .venv/bin/activate

# Verify activation
which python  # Should show .venv/bin/python
```

### Step 3: Download the Model (~40GB)

**Option A: Using the download script (recommended)**
```bash
./run.sh download-model
# or
python src/download_model.py --download
```

**Option B: Using huggingface-cli directly**
```bash
huggingface-cli download unsloth/Qwen3-Next-80B-A3B-Instruct-bnb-4bit \
  --local-dir models/qwen3-80b-bnb
```

**Verify the download:**
```bash
python src/download_model.py
# Should show "‚úì Model found locally!"
```

## üß™ Testing Workflows

### 1. Basic System Checks

#### Memory Check
```bash
./run.sh memory-check
```
Expected output:
- GPU detection and VRAM availability
- System RAM check
- Verification of requirements (‚úì or ‚úó)

#### Quick Configuration Test
```bash
./run.sh quick-test
```
This verifies:
- Configuration loading
- Module imports
- Basic setup validation

### 2. Unit Testing

#### Run All Tests
```bash
./run.sh test
# or
pytest tests/ -v
```

#### Run Specific Test Categories
```bash
# MoE setup tests
pytest tests/test_moe_setup.py -v

# Device mapping tests
pytest tests/test_device_mapping.py -v

# Expert caching tests
pytest tests/test_expert_caching.py -v

# API tests
pytest tests/test_api.py -v

# Performance tests
pytest tests/test_performance.py -v

# Inference pipeline tests
pytest tests/test_inference.py -v
```

#### Test Coverage Report
```bash
pytest tests/ --cov=src --cov-report=html
# Open htmlcov/index.html in browser
```

### 3. Inference Testing with curl

**‚úÖ CONFIRMED WORKING**: These curl commands have been tested and work successfully!

‚ö° **Loading Time**:
- **First load**: ~8-10 minutes (creates cache)
- **Subsequent loads**: ~22 seconds (from cache)

#### Simple Text Generation
```bash
# Generate text from command line
./run.sh generate "Explain quantum computing" --max-tokens 100 --temperature 0.7
```

#### Test Different Prompts
```bash
# Technical explanation
./run.sh generate "Write a Python function to implement quicksort" --max-tokens 200

# Creative writing
./run.sh generate "Once upon a time in a distant galaxy" --max-tokens 150 --temperature 0.9

# Question answering
./run.sh generate "What are the main differences between CPU and GPU?" --max-tokens 100 --temperature 0.5
```

### 4. API Server Testing

#### ‚ö° Model Loading Times
**First load: ~8-10 minutes | Cached load: ~22 seconds!**

Monitor loading progress:
```bash
# In terminal 1: Start the server
./run.sh serve

# In terminal 2: Monitor loading progress
tail -f server.log
# First load: "Loading checkpoint shards: X/9" (each takes ~80-150 seconds)
# Cached load: "Loading model from cache" (~22 seconds total)
```

#### Start the Server
```bash
# Start with default settings (wait ~22 seconds for cached load)
./run.sh serve

# Or with custom host/port
python main.py serve --host 0.0.0.0 --port 8000
```

The server will be available at `http://localhost:8000` **after model loads**
API documentation at `http://localhost:8000/docs`

### 5. ‚úÖ CURL Testing Examples (Tested & Working!)

These examples have been tested and confirmed working with the current implementation.

#### Quick Server Status Check
```bash
# Check if model is loaded and ready
curl http://localhost:8000/
# Expected when ready: {"service":"Qwen3-Local MoE API","version":"1.0.0","model":"qwen3-80b-moe-bnb","status":"ready"}
```

#### Simple Math Test (Recommended First Test)
```bash
# Test with 2+2 - Quick response, confirms model is working
curl -X POST "http://localhost:8000/api/v1/generate" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "2+2=",
    "max_tokens": 5,
    "temperature": 0.1
  }'
# Expected: {"text":"4","tokens_generated":1,"generation_time":~16,"expert_cache_hits":0}
```

#### Test API Endpoints

##### Basic Text Generation
```bash
# Simple greeting test
curl -X POST "http://localhost:8000/api/v1/generate" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Hello, my name is",
    "max_tokens": 10,
    "temperature": 0.7
  }'

# Question answering
curl -X POST "http://localhost:8000/api/v1/generate" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Question: What is the capital of France?\nAnswer:",
    "max_tokens": 20,
    "temperature": 0.3
  }'

# Code generation
curl -X POST "http://localhost:8000/api/v1/generate" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "def fibonacci(n):",
    "max_tokens": 100,
    "temperature": 0.5
  }'
```

##### OpenAI-Compatible Chat
```bash
# Simple chat completion
curl -X POST "http://localhost:8000/api/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3-80b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is 2+2?"}
    ],
    "max_tokens": 50,
    "temperature": 0.1
  }'

# More complex conversation
curl -X POST "http://localhost:8000/api/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3-80b",
    "messages": [
      {"role": "system", "content": "You are a helpful coding assistant."},
      {"role": "user", "content": "Write a Python function to check if a number is prime"}
    ],
    "max_tokens": 150,
    "temperature": 0.5
  }'
```

##### Streaming Response
```bash
curl -X POST "http://localhost:8000/api/v1/generate" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Write a short story about AI",
    "max_tokens": 200,
    "stream": true
  }'
```

##### Expert Statistics
```bash
# Get real-time expert usage statistics
curl "http://localhost:8000/api/v1/expert-stats"
```

##### Memory Statistics
```bash
# Monitor memory usage
curl "http://localhost:8000/api/v1/memory"
```

##### Model Information
```bash
# Get model details
curl "http://localhost:8000/api/v1/model-info"
```

#### Response Formatting with jq (Pretty Print)
```bash
# Install jq if not available: sudo pacman -S jq (Arch) or apt install jq (Debian/Ubuntu)

# Pretty print JSON response
curl -X POST "http://localhost:8000/api/v1/generate" \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Hello", "max_tokens": 5}' \
  -s | jq .
```

#### Timing Your Requests
```bash
# Measure response time for simple math
time curl -X POST "http://localhost:8000/api/v1/generate" \
  -H "Content-Type: application/json" \
  -d '{"prompt": "What is 2+2?", "max_tokens": 5, "temperature": 0.1}' \
  -s -o response.json && cat response.json | jq .
# Expected: ~16 seconds for inference (after model is loaded)
```

### 6. Performance Testing

#### Run Benchmarks
```bash
# Full benchmark suite
./run.sh benchmark --output results/benchmark_$(date +%Y%m%d)

# View results
cat results/benchmark_*/report.md
```

#### Profile Expert Usage
```bash
# Profile with 100 samples
./run.sh profile --samples 100

# This will show:
# - Most frequently used experts
# - Cache hit rates
# - VRAM usage patterns
```

#### Custom Benchmark Script
```python
# save as custom_benchmark.py
import sys
sys.path.insert(0, 'src')
from model_loader import ModelLoader
from inference import MoEInferencePipeline
from config import SystemConfig
import time

# Load model
config = SystemConfig()
config.model.local_model_path = "models/qwen3-80b-bnb"  # Use local path
loader = ModelLoader(config)
model, tokenizer = loader.load_model()

# Create pipeline
pipeline = MoEInferencePipeline(
    model=model,
    tokenizer=tokenizer,
    expert_manager=loader.expert_cache_manager
)

# Test prompts
prompts = [
    "Explain neural networks",
    "Write Python code for binary search",
    "What is quantum entanglement?"
]

# Benchmark
for prompt in prompts:
    start = time.time()
    output = pipeline.generate(prompt, max_new_tokens=100)
    elapsed = time.time() - start
    tokens = len(tokenizer.encode(output))
    print(f"Prompt: {prompt[:30]}...")
    print(f"Tokens/sec: {tokens/elapsed:.2f}")
    print("-" * 50)
```

### 6. Memory Optimization Testing

#### Test Different Expert Cache Sizes
```python
# save as test_cache_sizes.py
import json
from pathlib import Path

# Test configurations
configs = [
    {"experts_vram_gb": 2.0, "cached_experts_per_layer": 2},
    {"experts_vram_gb": 4.0, "cached_experts_per_layer": 3},
    {"experts_vram_gb": 6.0, "cached_experts_per_layer": 5}
]

for i, config in enumerate(configs):
    # Save config
    config_data = {
        "memory": config,
        "model": {"local_model_path": "models/qwen3-80b-bnb"}
    }
    with open(f"config_{i}.json", "w") as f:
        json.dump(config_data, f)

    # Run benchmark
    print(f"Testing config {i}: {config}")
    import subprocess
    subprocess.run([
        "python", "main.py", "benchmark",
        "--config", f"config_{i}.json",
        "--output", f"results/config_{i}"
    ])
```

#### Monitor Memory During Inference
```bash
# In terminal 1: Start monitoring
watch -n 1 nvidia-smi

# In terminal 2: Run inference
./run.sh generate "Long prompt requiring multiple experts..." --max-tokens 500
```

### 7. Stress Testing

#### Concurrent Requests
```python
# save as stress_test.py
import asyncio
import aiohttp
import time

async def make_request(session, prompt, request_id):
    url = "http://localhost:8000/api/v1/generate"
    data = {
        "prompt": prompt,
        "max_tokens": 50,
        "temperature": 0.7
    }

    start = time.time()
    async with session.post(url, json=data) as response:
        result = await response.json()
        elapsed = time.time() - start
        print(f"Request {request_id}: {elapsed:.2f}s")
        return result

async def stress_test(num_requests=10):
    prompts = [
        "Explain physics",
        "Write code",
        "Tell a story",
        "Solve math",
        "Translate text"
    ] * (num_requests // 5)

    async with aiohttp.ClientSession() as session:
        tasks = [make_request(session, prompt, i)
                for i, prompt in enumerate(prompts[:num_requests])]
        results = await asyncio.gather(*tasks)
        print(f"Completed {len(results)} requests")

# Run stress test
asyncio.run(stress_test(20))
```

#### Long Context Testing
```bash
# Test with increasing context lengths
for tokens in 100 500 1000 2000; do
  echo "Testing with $tokens tokens..."
  ./run.sh generate "Repeat this text many times: The quick brown fox jumps over the lazy dog." \
    --max-tokens $tokens --temperature 0.1
  sleep 5
done
```

## üîç Debugging and Troubleshooting

### Enable Debug Logging
```bash
# Set log level to DEBUG
python main.py generate "Test prompt" --log-level DEBUG

# Or for API server
python main.py serve --log-level DEBUG
```

### Check Model Loading
```python
# save as check_model.py
import sys
sys.path.insert(0, 'src')
from model_loader import ModelLoader
from config import SystemConfig
from pathlib import Path

config = SystemConfig()
config.model.local_model_path = "models/qwen3-80b-bnb"
config.model_dir = Path("models/qwen3-80b-bnb")
loader = ModelLoader(config)

print("Loading model from:", config.model_dir)
model, tokenizer = loader.load_model()

print("\nModel info:")
print(f"Model type: {type(model)}")
print(f"Device map keys: {list(model.hf_device_map.keys())[:5]}...")
print(f"Tokenizer vocab size: {len(tokenizer)}")

# Check expert placement
expert_count = 0
gpu_experts = 0
cpu_experts = 0

for name, device in model.hf_device_map.items():
    if "expert" in name:
        expert_count += 1
        if device == 0:
            gpu_experts += 1
        else:
            cpu_experts += 1

print(f"\nExpert distribution:")
print(f"Total experts: {expert_count}")
print(f"GPU experts: {gpu_experts}")
print(f"CPU experts: {cpu_experts}")
```

### Monitor Expert Swapping
```python
# save as monitor_experts.py
import sys
sys.path.insert(0, 'src')
from model_loader import ModelLoader
from inference import MoEInferencePipeline
from config import SystemConfig
import logging

config = SystemConfig()
config.model.local_model_path = "models/qwen3-80b-bnb"
loader = ModelLoader(config)
model, tokenizer = loader.load_model()

pipeline = MoEInferencePipeline(
    model=model,
    tokenizer=tokenizer,
    expert_manager=loader.expert_cache_manager
)

# Enable detailed logging
logging.basicConfig(level=logging.DEBUG)

# Generate text and watch expert swaps
output = pipeline.generate(
    "Complex prompt requiring many experts",
    max_new_tokens=100,
    optimize_experts=True
)

# Print expert stats
stats = loader.expert_cache_manager.get_cache_stats()
print("\nExpert usage statistics:")
print(f"Cache hits: {stats.get('cache_hits', 0)}")
print(f"Cache misses: {stats.get('cache_misses', 0)}")
print(f"Hit rate: {stats.get('cache_hit_rate', 0):.2%}")
```

## üéØ Creating Your Own Experiments

### 1. Custom Configuration
```json
{
  "model": {
    "model_name": "unsloth/Qwen3-Next-80B-A3B-Instruct-bnb-4bit",
    "local_model_path": "models/qwen3-80b-bnb"
  },
  "memory": {
    "gpu_memory_gb": 14.0,
    "cpu_memory_gb": 90.0,
    "experts_vram_gb": 4.0,
    "cached_experts_per_layer": 3
  },
  "inference": {
    "max_batch_size": 4,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.1
  }
}
```

Use custom config:
```bash
python main.py generate "Test" --config custom_config.json
```

### 2. Custom Expert Strategy
```python
# save as custom_expert_strategy.py
import sys
sys.path.insert(0, 'src')
from expert_manager import ExpertCacheManager

class CustomExpertManager(ExpertCacheManager):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def get_experts_to_cache(self, layer_idx: int):
        """Custom logic for expert selection"""
        # Example: Cache more experts for middle layers
        if 20 <= layer_idx <= 60:
            return 5  # Cache 5 experts
        else:
            return 3  # Cache 3 experts

    def should_swap_expert(self, expert_id: str, usage_count: int):
        """Custom swap decision"""
        # Example: More aggressive swapping
        threshold = 10  # Swap if not used in last 10 calls
        recent_uses = self.get_recent_usage(expert_id, window=10)
        return recent_uses < threshold
```

## üìä Performance Expectations

### ‚úÖ Actual Measured Performance (RTX 4090 + 120GB RAM)
- **First Model Load**: ~8-10 minutes (creates 40GB cache file)
- **Cached Model Load**: **~22 seconds** ‚ö°
- **Inference Speed**: ~0.12 tokens/second (CPU-based execution)
- **Simple Math (2+2)**: ~16 seconds total generation time
- **Memory Usage**: ~8GB VRAM (embeddings), ~55-60GB RAM (model weights)
- **Cache File Size**: 40GB (enables fast restarts)

### Optimization Tips
1. **Lower temperature** (0.5-0.7) for more predictable expert routing
2. **Batch similar prompts** to maximize cache hits
3. **Profile common use cases** to optimize expert caching
4. **Adjust cache size** based on available VRAM
5. **Use streaming** for better perceived performance

## üêõ Common Issues and Solutions

### ‚úÖ Issue: Model Loading (FIXED - Now ~22 seconds with caching!)
```bash
# First load: ~8-10 minutes (normal for 40GB model)
# Progress indicator: "Loading checkpoint shards: X/9" (each ~80-150 seconds)
# Subsequent loads: ~22 seconds from cache!

# Cache management:
./run.sh cache --info    # Check cache status
./run.sh cache --clear   # Clear cache if needed
```

### Issue: Model Not Found
```bash
# Error: Model not found at models/qwen3-80b-bnb
# Solution: Download the model first
./run.sh download-model
```

### ‚úÖ Issue: CUDA Out of Memory (FIXED - Model runs on CPU)
```python
# The model now uses CPU-based execution to avoid meta tensor issues
# VRAM is only used for embeddings (~8GB)
# If you still get OOM, check what else is using GPU:
nvidia-smi  # Check for other processes using VRAM
```

### Issue: Slow First Inference
```python
# Solution: Warmup the model
pipeline.generate("Hello", max_new_tokens=10)  # Warmup
# Now ready for actual use
```

### Issue: Import Errors
```bash
# Solution: Ensure virtual environment is activated
source .venv/bin/activate  # Note: .venv not venv
# Reinstall if needed
./install.sh
```

### Issue: Permission Denied on Scripts
```bash
# Solution: Make scripts executable
chmod +x run.sh install.sh
```

## üéì Learning Resources

### Understanding the Architecture
1. Review `src/moe_utils.py` - Core MoE device mapping
2. Study `src/expert_manager.py` - Expert caching logic
3. Examine `src/inference.py` - Inference pipeline
4. Check `.claude/CLAUDE.md` for implementation details

### Key Concepts
- **MoE (Mixture of Experts)**: 80B parameters, 3B active per forward pass
- **Hybrid Execution**: Non-experts in VRAM, experts in RAM
- **Dynamic Caching**: Profile-based expert placement
- **BitsAndBytes**: 4-bit quantization for memory efficiency

## üöÄ Next Steps

After familiarizing yourself with the testing workflows:

1. **Verify model download** - Ensure model is in `models/qwen3-80b-bnb/`
2. **Run memory check** - Confirm your hardware meets requirements
3. **Try simple generation** - Start with basic prompts
4. **Benchmark your hardware** - Run full benchmark suite
5. **Profile your use case** - Identify common expert patterns
6. **Optimize configuration** - Tune for your specific needs
7. **Create custom experiments** - Build on the foundation

## üìù Important Notes

- **Virtual environment**: Always use `.venv` (not `venv`)
- **Model location**: Local model at `models/qwen3-80b-bnb/` (~40GB)
- **Package manager**: Use `uv pip` instead of `pip`
- **Memory monitoring**: Keep `nvidia-smi` running during tests
- **Configuration**: Check `.claude/CLAUDE.md` for details
- **Model source**: Alibaba's Qwen3-Next-80B quantized by Unsloth

## ‚ö†Ô∏è Safety Reminders

- **Never run tests without checking memory first** - OOM can crash system
- **Start with small `max_tokens` values** - Increase gradually
- **Monitor GPU temperature** - Heavy load can cause thermal throttling
- **Save work before testing** - In case of system instability
- **Use lower batch sizes initially** - Scale up after verification

---

*Happy experimenting with Qwen3-80B MoE implementation! üöÄ*

*For issues, see: https://github.com/PieBru/Qwen3-NEXT-80B-Tests/issues*
