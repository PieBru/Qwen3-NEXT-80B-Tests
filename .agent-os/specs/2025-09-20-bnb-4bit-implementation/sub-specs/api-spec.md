# API Specification

This is the API specification for the spec detailed in @.agent-os/specs/2025-09-20-bnb-4bit-implementation/spec.md

> Created: 2025-09-20
> Version: 1.0.0

## Endpoints

### POST /api/v1/generate

**Purpose:** Generate text completion using BitsAndBytes 4-bit quantized Qwen3 model
**Parameters:**
- `prompt` (string, required): Input text for generation
- `max_tokens` (integer, optional, default: 2048): Maximum tokens to generate
- `temperature` (float, optional, default: 0.7): Sampling temperature
- `top_p` (float, optional, default: 0.8): Top-p (nucleus) sampling parameter
- `top_k` (integer, optional, default: 20): Top-k sampling parameter
- `stream` (boolean, optional, default: false): Enable streaming response

**Response:** JSON with generated text, token counts, and generation metadata
**Errors:** 400 (invalid parameters), 413 (context too long), 500 (generation error)

### POST /api/v1/chat/completions

**Purpose:** Chat completion compatible with OpenAI API format
**Parameters:**
- `messages` (array, required): Chat message history
- `model` (string, optional): Model identifier
- `max_tokens` (integer, optional, default: 2048): Maximum completion tokens
- `temperature` (float, optional, default: 0.7): Sampling temperature
- `top_p` (float, optional, default: 0.8): Top-p sampling
- `stream` (boolean, optional, default: false): Streaming response

**Response:** OpenAI-compatible chat completion response format
**Errors:** 400 (malformed messages), 413 (context exceeded), 500 (inference error)

### GET /api/v1/model/info

**Purpose:** Retrieve model information and configuration details
**Parameters:** None
**Response:** Model name, quantization info, memory usage, context window size
**Errors:** 500 (model not loaded)

### GET /api/v1/health

**Purpose:** Health check endpoint for monitoring
**Parameters:** None
**Response:** Service status, model load status, memory utilization
**Errors:** 503 (service unavailable)

## Controllers

### BnBGenerationController

**generate_text()**
- Loads and processes input prompt with proper tokenization
- Applies BitsAndBytes 4-bit model inference with configured sampling parameters
- Handles context window management and truncation for inputs up to 262K tokens
- Returns generated text with metadata (token counts, generation time)

**stream_generate()**
- Implements Server-Sent Events (SSE) for streaming text generation
- Yields partial completions as they are generated by the quantized model
- Manages connection lifecycle and error handling during streaming
- Applies proper token-by-token or chunk-based streaming strategies

### ChatController

**create_completion()**
- Processes chat message arrays into proper prompt format for Qwen3
- Applies chat template formatting and system message handling
- Manages conversation context and memory within 262K token limits
- Returns OpenAI-compatible response structure with usage statistics

**handle_function_calls()**
- Processes tool/function calling requests if supported by the model
- Manages structured output generation for API integrations
- Handles function call parsing and response formatting

### ModelController

**load_model()**
- Initializes BitsAndBytes 4-bit quantized model from unsloth repository
- Configures device mapping and memory optimization settings
- Validates model loading and quantization application
- Sets up tokenizer with proper special tokens and padding configuration

**get_model_info()**
- Returns comprehensive model metadata including quantization details
- Provides memory usage statistics and performance metrics
- Reports context window capabilities and current model status